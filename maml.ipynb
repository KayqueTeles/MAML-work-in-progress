{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.13.2\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install opencv-python\n",
    "!pip install tqdm\n",
    "!pip install Pillow\n",
    "!pip install miniimagenettools\n",
    "\n",
    "\n",
    "# Utils Imports\n",
    "\"\"\" Utility functions. \"\"\"\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from tensorflow.contrib.layers.python import layers as tf_layers\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "#\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Utils-Início\n",
    "## Image helper\n",
    "def get_images(paths, labels, nb_samples=None, shuffle=True):\n",
    "    if nb_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, nb_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images = [(i, os.path.join(path, image)) \\\n",
    "        for i, path in zip(labels, paths) \\\n",
    "        for image in sampler(os.listdir(path))]\n",
    "    if shuffle:\n",
    "        random.shuffle(images)\n",
    "    return images\n",
    "\n",
    "## Network helpers\n",
    "def conv_block(inp, cweight, bweight, reuse, scope, activation=tf.nn.relu, max_pool_pad='VALID', residual=False):\n",
    "    \"\"\" Perform, conv, batch norm, nonlinearity, and max pool \"\"\"\n",
    "    stride, no_stride = [1,2,2,1], [1,1,1,1]\n",
    "\n",
    "    if FLAGS.max_pool:\n",
    "        conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight\n",
    "    else:\n",
    "        conv_output = tf.nn.conv2d(inp, cweight, stride, 'SAME') + bweight\n",
    "    normed = normalize(conv_output, activation, reuse, scope)\n",
    "    if FLAGS.max_pool:\n",
    "        normed = tf.nn.max_pool(normed, stride, stride, max_pool_pad)\n",
    "    return normed\n",
    "\n",
    "def normalize(inp, activation, reuse, scope):\n",
    "    if FLAGS.norm == 'batch_norm':\n",
    "        return tf_layers.batch_norm(inp, activation_fn=activation, reuse=reuse, scope=scope)\n",
    "    elif FLAGS.norm == 'layer_norm':\n",
    "        return tf_layers.layer_norm(inp, activation_fn=activation, reuse=reuse, scope=scope)\n",
    "    elif FLAGS.norm == 'None':\n",
    "        if activation is not None:\n",
    "            return activation(inp)\n",
    "        else:\n",
    "            return inp\n",
    "\n",
    "## Loss functions\n",
    "def mse(pred, label):\n",
    "    pred = tf.reshape(pred, [-1])\n",
    "    label = tf.reshape(label, [-1])\n",
    "    return tf.reduce_mean(tf.square(pred-label))\n",
    "\n",
    "def xent(pred, label):\n",
    "    # Note - with tf version <=0.12, this loss has incorrect 2nd derivatives\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=label) / FLAGS.update_batch_size\n",
    "\n",
    "# Utils-Fim\n",
    "\n",
    "\n",
    "# Data-Generator-Início\n",
    "\"\"\" Code for loading data. \"\"\"\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import random\n",
    "# import tensorflow as tf\n",
    "\n",
    "# from tensorflow.python.platform import flags\n",
    "from utils import get_images\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of sinusoid or Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits or a particular sinusoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples_per_class, batch_size, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples_per_class: num samples to generate per class in one batch\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        print('\\n ** Initializing data_generator at data_generator.py ...')\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = 1  # by default 1 (only relevant for classification problems)\n",
    "\n",
    "\n",
    "            print(' ** Mini-Imagenet predefinitions selected.')\n",
    "            self.num_classes = config.get('num_classes', FLAGS.num_classes)\n",
    "            print(' ** Defining image data...')\n",
    "            self.img_size = config.get('img_size', (84, 84))\n",
    "            # self.img_size = config.get('img_size', (101, 101))\n",
    "            self.dim_input = np.prod(self.img_size)*3\n",
    "            self.dim_output = self.num_classes\n",
    "            print(' ** Choosing folders...')\n",
    "\n",
    "            # data-gdrive = wget https://colab.research.google.com/drive/1UPwWzkH0ip_9rCswNn5p9iFzqY50_u4T?usp=sharing\n",
    "            data-gdrive-zip = !wget \"https://colab.research.google.com/drive/1UPwWzkH0ip_9rCswNn5p9iFzqY50_u4T?usp=sharing\"\n",
    "            data-gdrive = !unzip data-gdrive-zip\n",
    "\n",
    "\n",
    "            dgd = !unzip \"/content/drive/My Drive/Tcc/miniImagenet.zip\"\n",
    "\n",
    "\n",
    "            #metatrain_folder = config.get('metatrain_folder', './data/miniImagenet/train')\n",
    "            #if FLAGS.test_set:\n",
    "            #    metaval_folder = config.get('metaval_folder', './data/miniImagenet/test')\n",
    "            #else:\n",
    "            #    metaval_folder = config.get('metaval_folder', './data/miniImagenet/val')\n",
    "\n",
    "            metatrain_folder = config.get('metatrain_folder', './dgd/data/miniImagenet/train')\n",
    "            if FLAGS.test_set:\n",
    "                metaval_folder = config.get('metaval_folder', './dgd/data/miniImagenet/test')\n",
    "            else:\n",
    "                metaval_folder = config.get('metaval_folder', './dgd/data/miniImagenet/val')\n",
    "\n",
    "\n",
    "            print(' ** Defining folder labels...')\n",
    "            metatrain_folders = [os.path.join(metatrain_folder, label) \\\n",
    "                for label in os.listdir(metatrain_folder) \\\n",
    "                if os.path.isdir(os.path.join(metatrain_folder, label)) \\\n",
    "                ]\n",
    "            metaval_folders = [os.path.join(metaval_folder, label) \\\n",
    "                for label in os.listdir(metaval_folder) \\\n",
    "                if os.path.isdir(os.path.join(metaval_folder, label)) \\\n",
    "                ]\n",
    "            print(' ** Checking if data source is reckognized...')\n",
    "            self.metatrain_character_folders = metatrain_folders\n",
    "            self.metaval_character_folders = metaval_folders\n",
    "            self.rotations = config.get('rotations', [0])\n",
    "        else:\n",
    "            raise ValueError('Unrecognized data source')\n",
    "\n",
    "\n",
    "    def make_data_tensor(self, train=True):\n",
    "        print(' ** Creating data tensor...')\n",
    "        if train:\n",
    "            folders = self.metatrain_character_folders\n",
    "            # number of tasks, not number of meta-iterations. (divide by metabatch size to measure)\n",
    "            num_total_batches = 200000\n",
    "        else:\n",
    "            folders = self.metaval_character_folders\n",
    "            num_total_batches = 600\n",
    "\n",
    "        # make list of files\n",
    "        print(' ** Generating filenames....')\n",
    "        all_filenames = []\n",
    "        for _ in range(num_total_batches):\n",
    "            sampled_character_folders = random.sample(folders, self.num_classes)\n",
    "            random.shuffle(sampled_character_folders)\n",
    "            labels_and_images = get_images(sampled_character_folders, range(self.num_classes), nb_samples=self.num_samples_per_class, shuffle=False)\n",
    "            # make sure the above isn't randomized order\n",
    "            labels = [li[0] for li in labels_and_images]\n",
    "            filenames = [li[1] for li in labels_and_images]\n",
    "            all_filenames.extend(filenames)\n",
    "\n",
    "        print(' ** Generating filename queue....')\n",
    "        # make queue for tensorflow to read from\n",
    "        filename_queue = tf.train.string_input_producer(tf.convert_to_tensor(all_filenames), shuffle=False)\n",
    "        print(' ** Generating image processing ops')\n",
    "        image_reader = tf.WholeFileReader()\n",
    "        _, image_file = image_reader.read(filename_queue)\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "            image.set_shape((self.img_size[0],self.img_size[1],3))\n",
    "            image = tf.reshape(image, [self.dim_input])\n",
    "            image = tf.cast(image, tf.float32) / 255.0\n",
    "        else:\n",
    "            image = tf.image.decode_png(image_file)\n",
    "            image.set_shape((self.img_size[0],self.img_size[1],1))\n",
    "            image = tf.reshape(image, [self.dim_input])\n",
    "            image = tf.cast(image, tf.float32) / 255.0\n",
    "            image = 1.0 - image  # invert\n",
    "        num_preprocess_threads = 1 # TODO - enable this to be set to >1\n",
    "        min_queue_examples = 256\n",
    "        examples_per_batch = self.num_classes * self.num_samples_per_class\n",
    "        batch_image_size = self.batch_size  * examples_per_batch\n",
    "        print(' ** Batching images')\n",
    "        images = tf.train.batch(\n",
    "                [image],\n",
    "                batch_size = batch_image_size,\n",
    "                num_threads=num_preprocess_threads,\n",
    "                capacity=min_queue_examples + 3 * batch_image_size,\n",
    "                )\n",
    "        all_image_batches, all_label_batches = [], []\n",
    "        print(' ** Manipulating image data to be right shape')\n",
    "        for i in range(self.batch_size):\n",
    "            image_batch = images[i*examples_per_batch:(i+1)*examples_per_batch]\n",
    "\n",
    "            if FLAGS.datasource == 'omniglot':\n",
    "                # omniglot augments the dataset by rotating digits to create new classes\n",
    "                # get rotation per class (e.g. 0,1,2,0,0 if there are 5 classes)\n",
    "                rotations = tf.multinomial(tf.log([[1., 1.,1.,1.]]), self.num_classes)\n",
    "            label_batch = tf.convert_to_tensor(labels)\n",
    "            new_list, new_label_list = [], []\n",
    "            for k in range(self.num_samples_per_class):\n",
    "                class_idxs = tf.range(0, self.num_classes)\n",
    "                class_idxs = tf.random_shuffle(class_idxs)\n",
    "\n",
    "                true_idxs = class_idxs*self.num_samples_per_class + k\n",
    "                new_list.append(tf.gather(image_batch,true_idxs))\n",
    "                if FLAGS.datasource == 'omniglot': # and FLAGS.train:\n",
    "                    new_list[-1] = tf.stack([tf.reshape(tf.image.rot90(\n",
    "                        tf.reshape(new_list[-1][ind], [self.img_size[0],self.img_size[1],1]),\n",
    "                        k=tf.cast(rotations[0,class_idxs[ind]], tf.int32)), (self.dim_input,))\n",
    "                        for ind in range(self.num_classes)])\n",
    "                new_label_list.append(tf.gather(label_batch, true_idxs))\n",
    "            new_list = tf.concat(new_list, 0)  # has shape [self.num_classes*self.num_samples_per_class, self.dim_input]\n",
    "            new_label_list = tf.concat(new_label_list, 0)\n",
    "            all_image_batches.append(new_list)\n",
    "            all_label_batches.append(new_label_list)\n",
    "        all_image_batches = tf.stack(all_image_batches)\n",
    "        all_label_batches = tf.stack(all_label_batches)\n",
    "        all_label_batches = tf.one_hot(all_label_batches, self.num_classes)\n",
    "        print(' ** Data tensor generation complete.')\n",
    "        return all_image_batches, all_label_batches\n",
    "\n",
    "    def generate_sinusoid_batch(self, train=True, input_idx=None):\n",
    "        # Note train arg is not used (but it is used for omniglot method.\n",
    "        # input_idx is used during qualitative testing --the number of examples used for the grad update\n",
    "        amp = np.random.uniform(self.amp_range[0], self.amp_range[1], [self.batch_size])\n",
    "        phase = np.random.uniform(self.phase_range[0], self.phase_range[1], [self.batch_size])\n",
    "        outputs = np.zeros([self.batch_size, self.num_samples_per_class, self.dim_output])\n",
    "        init_inputs = np.zeros([self.batch_size, self.num_samples_per_class, self.dim_input])\n",
    "        for func in range(self.batch_size):\n",
    "            init_inputs[func] = np.random.uniform(self.input_range[0], self.input_range[1], [self.num_samples_per_class, 1])\n",
    "            if input_idx is not None:\n",
    "                init_inputs[:,input_idx:,0] = np.linspace(self.input_range[0], self.input_range[1], num=self.num_samples_per_class-input_idx, retstep=False)\n",
    "            outputs[func] = amp[func] * np.sin(init_inputs[func]-phase[func])\n",
    "        return init_inputs, outputs, amp, phase\n",
    "\n",
    "# Data-Generator-Fim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MAML-Início\n",
    "\"\"\" Code for the MAML algorithm and network definitions. \"\"\"\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import special_grads\n",
    "except KeyError as e:\n",
    "    print('WARN: Cannot define MaxPoolGrad, likely already defined for this version of tensorflow: %s' % e,\n",
    "          file=sys.stderr)\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "from utils import mse, xent, conv_block, normalize\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "class MAML:\n",
    "    def __init__(self, dim_input=1, dim_output=1, test_num_updates=5):\n",
    "        \"\"\" must call construct_model() after initializing MAML! \"\"\"\n",
    "        print('\\n ** Initializing model generator at maml.py ...')\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "        self.update_lr = FLAGS.update_lr\n",
    "        print(' ** Defining model aspects.')\n",
    "        self.meta_lr = tf.placeholder_with_default(FLAGS.meta_lr, ())\n",
    "        self.classification = False\n",
    "        self.test_num_updates = test_num_updates\n",
    "        if FLAGS.datasource == 'sinusoid':\n",
    "            self.dim_hidden = [40, 40]\n",
    "            self.loss_func = mse\n",
    "            self.forward = self.forward_fc\n",
    "            self.construct_weights = self.construct_fc_weights\n",
    "        elif FLAGS.datasource == 'omniglot' or FLAGS.datasource == 'miniimagenet':\n",
    "            print(' ** Defining model weights.')\n",
    "            self.loss_func = xent\n",
    "            self.classification = True\n",
    "            if FLAGS.conv:\n",
    "                self.dim_hidden = FLAGS.num_filters\n",
    "                self.forward = self.forward_conv\n",
    "                self.construct_weights = self.construct_conv_weights\n",
    "            else:\n",
    "                self.dim_hidden = [256, 128, 64, 64]\n",
    "                self.forward=self.forward_fc\n",
    "                self.construct_weights = self.construct_fc_weights\n",
    "            if FLAGS.datasource == 'miniimagenet':\n",
    "                self.channels = 3\n",
    "            else:\n",
    "                self.channels = 1\n",
    "            self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
    "            print(' ** Finished model weights generation.')\n",
    "        else:\n",
    "            raise ValueError('Unrecognized data source.')\n",
    "\n",
    "    def construct_model(self, input_tensors=None, prefix='metatrain_'):\n",
    "        # a: training data for inner gradient, b: test data for meta gradient\n",
    "        print(' ** Constructing model at maml.py ...')\n",
    "        if input_tensors is None:\n",
    "            self.inputa = tf.placeholder(tf.float32)\n",
    "            self.inputb = tf.placeholder(tf.float32)\n",
    "            self.labela = tf.placeholder(tf.float32)\n",
    "            self.labelb = tf.placeholder(tf.float32)\n",
    "            print(' ** INPUT_TENSORS IS NONE!')\n",
    "        else:\n",
    "            self.inputa = input_tensors['inputa']\n",
    "            self.inputb = input_tensors['inputb']\n",
    "            self.labela = input_tensors['labela']\n",
    "            self.labelb = input_tensors['labelb']\n",
    "            print(' ** INPUT_TENSORS ISNT NONE!')\n",
    "        #PLACEHOLDER = UMA VARIaVEL QUE FICA GUARDADA ATe SER DEFINIDA POSTERIORMENTE\n",
    "\n",
    "\n",
    "        ############COMMENTING THIS WILL ALLOW US TO MAKE IT DO NOT REUSE WEIGHTS, I HOPE\n",
    "        with tf.variable_scope('model', reuse=None) as training_scope:\n",
    "            if 'weights' in dir(self):\n",
    "                training_scope.reuse_variables()\n",
    "                weights = self.weights\n",
    "                print(' ** WEIGHTS IN DIR(SELF)!')\n",
    "            else:\n",
    "                # Define the weights\n",
    "                self.weights = weights = self.construct_weights()\n",
    "                print(' ** WEIGHTS ISNT!')\n",
    "\n",
    "            # outputbs[i] and lossesb[i] is the output and loss after i+1 gradient updates\n",
    "            lossesa, outputas, lossesb, outputbs = [], [], [], []\n",
    "            accuraciesa, accuraciesb = [], []\n",
    "            num_updates = max(self.test_num_updates, FLAGS.num_updates)\n",
    "            outputbs = [[]]*num_updates\n",
    "            lossesb = [[]]*num_updates\n",
    "            accuraciesb = [[]]*num_updates\n",
    "\n",
    "            def task_metalearn(inp, reuse=True):\n",
    "                print(' ** Performing gradient descent...')\n",
    "                \"\"\" Perform gradient descent for one task in the meta-batch. \"\"\"\n",
    "                inputa, inputb, labela, labelb = inp\n",
    "                task_outputbs, task_lossesb = [], []\n",
    "\n",
    "                if self.classification:\n",
    "                    task_accuraciesb = []\n",
    "\n",
    "                print(' ** Task outputa phase.')\n",
    "                #task_outputa = self.forward(inputa, weights, reuse=True)  # only reuse on the first iter\n",
    "                task_outputa = self.forward(inputa, weights, reuse=reuse)  # only reuse on the first iter\n",
    "                print(' ** Task lossa phase.')\n",
    "                task_lossa = self.loss_func(task_outputa, labela)\n",
    "                print(' ** Defining gradients...')\n",
    "                grads = tf.gradients(task_lossa, list(weights.values()))\n",
    "                if FLAGS.stop_grad:\n",
    "                    grads = [tf.stop_gradient(grad) for grad in grads]\n",
    "                gradients = dict(zip(weights.keys(), grads))\n",
    "                fast_weights = dict(zip(weights.keys(), [weights[key] - self.update_lr*gradients[key] for key in weights.keys()]))\n",
    "                output = self.forward(inputb, fast_weights, reuse=True)\n",
    "                task_outputbs.append(output)\n",
    "                task_lossesb.append(self.loss_func(output, labelb))\n",
    "\n",
    "                for j in range(num_updates - 1):\n",
    "                    loss = self.loss_func(self.forward(inputa, fast_weights, reuse=True), labela)\n",
    "                    grads = tf.gradients(loss, list(fast_weights.values()))\n",
    "                    if FLAGS.stop_grad:\n",
    "                        grads = [tf.stop_gradient(grad) for grad in grads]\n",
    "                    gradients = dict(zip(fast_weights.keys(), grads))\n",
    "                    fast_weights = dict(zip(fast_weights.keys(), [fast_weights[key] - self.update_lr*gradients[key] for key in fast_weights.keys()]))\n",
    "                    output = self.forward(inputb, fast_weights, reuse=True)\n",
    "                    task_outputbs.append(output)\n",
    "                    task_lossesb.append(self.loss_func(output, labelb))\n",
    "\n",
    "                task_output = [task_outputa, task_outputbs, task_lossa, task_lossesb]\n",
    "\n",
    "                if self.classification:\n",
    "                    task_accuracya = tf.contrib.metrics.accuracy(tf.argmax(tf.nn.softmax(task_outputa), 1), tf.argmax(labela, 1))\n",
    "                    for j in range(num_updates):\n",
    "                        task_accuraciesb.append(tf.contrib.metrics.accuracy(tf.argmax(tf.nn.softmax(task_outputbs[j]), 1), tf.argmax(labelb, 1)))\n",
    "                    task_output.extend([task_accuracya, task_accuraciesb])\n",
    "\n",
    "                return task_output\n",
    "\n",
    "            if FLAGS.norm is not 'None':\n",
    "                # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
    "                unused = task_metalearn((self.inputa[0], self.inputb[0], self.labela[0], self.labelb[0]), False)\n",
    "\n",
    "            out_dtype = [tf.float32, [tf.float32]*num_updates, tf.float32, [tf.float32]*num_updates]\n",
    "            if self.classification:\n",
    "                out_dtype.extend([tf.float32, [tf.float32]*num_updates])\n",
    "            result = tf.map_fn(task_metalearn, elems=(self.inputa, self.inputb, self.labela, self.labelb), dtype=out_dtype, parallel_iterations=FLAGS.meta_batch_size)\n",
    "            if self.classification:\n",
    "                outputas, outputbs, lossesa, lossesb, accuraciesa, accuraciesb = result\n",
    "            else:\n",
    "                outputas, outputbs, lossesa, lossesb  = result\n",
    "\n",
    "        ## Performance & Optimization\n",
    "        if 'train' in prefix:\n",
    "            self.total_loss1 = total_loss1 = tf.reduce_sum(lossesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "            self.total_losses2 = total_losses2 = [tf.reduce_sum(lossesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "            # after the map_fn\n",
    "            self.outputas, self.outputbs = outputas, outputbs\n",
    "            if self.classification:\n",
    "                self.total_accuracy1 = total_accuracy1 = tf.reduce_sum(accuraciesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "                self.total_accuracies2 = total_accuracies2 = [tf.reduce_sum(accuraciesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "            self.pretrain_op = tf.train.AdamOptimizer(self.meta_lr).minimize(total_loss1)\n",
    "\n",
    "            if FLAGS.metatrain_iterations > 0:\n",
    "                optimizer = tf.train.AdamOptimizer(self.meta_lr)\n",
    "                self.gvs = gvs = optimizer.compute_gradients(self.total_losses2[FLAGS.num_updates-1])\n",
    "                if FLAGS.datasource == 'miniimagenet':\n",
    "                    gvs = [(tf.clip_by_value(grad, -10, 10), var) for grad, var in gvs]\n",
    "                self.metatrain_op = optimizer.apply_gradients(gvs)\n",
    "        else:\n",
    "            self.metaval_total_loss1 = total_loss1 = tf.reduce_sum(lossesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "            self.metaval_total_losses2 = total_losses2 = [tf.reduce_sum(lossesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "            if self.classification:\n",
    "                self.metaval_total_accuracy1 = total_accuracy1 = tf.reduce_sum(accuraciesa) / tf.to_float(FLAGS.meta_batch_size)\n",
    "                self.metaval_total_accuracies2 = total_accuracies2 =[tf.reduce_sum(accuraciesb[j]) / tf.to_float(FLAGS.meta_batch_size) for j in range(num_updates)]\n",
    "\n",
    "        ## Summaries\n",
    "        tf.summary.scalar(prefix+'Pre-update loss', total_loss1)\n",
    "        if self.classification:\n",
    "            tf.summary.scalar(prefix+'Pre-update accuracy', total_accuracy1)\n",
    "\n",
    "        for j in range(num_updates):\n",
    "            tf.summary.scalar(prefix+'Post-update loss, step ' + str(j+1), total_losses2[j])\n",
    "            if self.classification:\n",
    "                tf.summary.scalar(prefix+'Post-update accuracy, step ' + str(j+1), total_accuracies2[j])\n",
    "\n",
    "    ### Network construction functions (fc networks and conv networks)\n",
    "    def construct_fc_weights(self):\n",
    "        weights = {}\n",
    "        weights['w1'] = tf.Variable(tf.truncated_normal([self.dim_input, self.dim_hidden[0]], stddev=0.01))\n",
    "        weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden[0]]))\n",
    "        for i in range(1,len(self.dim_hidden)):\n",
    "            weights['w'+str(i+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[i-1], self.dim_hidden[i]], stddev=0.01))\n",
    "            weights['b'+str(i+1)] = tf.Variable(tf.zeros([self.dim_hidden[i]]))\n",
    "        weights['w'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.truncated_normal([self.dim_hidden[-1], self.dim_output], stddev=0.01))\n",
    "        weights['b'+str(len(self.dim_hidden)+1)] = tf.Variable(tf.zeros([self.dim_output]))\n",
    "        return weights\n",
    "\n",
    "    def forward_fc(self, inp, weights, reuse=False):\n",
    "        #print(' ** IS THE PROBLEM HERE??????')  #NO, ITS NOT!\n",
    "        hidden = normalize(tf.matmul(inp, weights['w1']) + weights['b1'], activation=tf.nn.relu, reuse=reuse, scope='0')\n",
    "        for i in range(1,len(self.dim_hidden)):\n",
    "            hidden = normalize(tf.matmul(hidden, weights['w'+str(i+1)]) + weights['b'+str(i+1)], activation=tf.nn.relu, reuse=reuse, scope=str(i+1))\n",
    "        return tf.matmul(hidden, weights['w'+str(len(self.dim_hidden)+1)]) + weights['b'+str(len(self.dim_hidden)+1)]\n",
    "\n",
    "    def construct_conv_weights(self):\n",
    "        weights = {}\n",
    "\n",
    "        dtype = tf.float32\n",
    "        conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)\n",
    "        fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=dtype)\n",
    "        k = 3\n",
    "\n",
    "        weights['conv1'] = tf.get_variable('conv1', [k, k, self.channels, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        weights['conv2'] = tf.get_variable('conv2', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b2'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        weights['conv3'] = tf.get_variable('conv3', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b3'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        weights['conv4'] = tf.get_variable('conv4', [k, k, self.dim_hidden, self.dim_hidden], initializer=conv_initializer, dtype=dtype)\n",
    "        weights['b4'] = tf.Variable(tf.zeros([self.dim_hidden]))\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            # assumes max pooling\n",
    "            weights['w5'] = tf.get_variable('w5', [self.dim_hidden*5*5, self.dim_output], initializer=fc_initializer)\n",
    "            weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
    "        else:\n",
    "            weights['w5'] = tf.Variable(tf.random_normal([self.dim_hidden, self.dim_output]), name='w5')\n",
    "            weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
    "\n",
    "        return weights\n",
    "\n",
    "    def forward_conv(self, inp, weights, reuse=False, scope=''):\n",
    "        print(' ** Forward_conv being applied...')\n",
    "        # reuse is for the normalization parameters.\n",
    "        channels = self.channels\n",
    "        inp = tf.reshape(inp, [-1, self.img_size, self.img_size, channels])\n",
    "\n",
    "        hidden1 = conv_block(inp, weights['conv1'], weights['b1'], reuse, scope+'0')\n",
    "        hidden2 = conv_block(hidden1, weights['conv2'], weights['b2'], reuse, scope+'1')\n",
    "        hidden3 = conv_block(hidden2, weights['conv3'], weights['b3'], reuse, scope+'2')\n",
    "        hidden4 = conv_block(hidden3, weights['conv4'], weights['b4'], reuse, scope+'3')\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            # last hidden layer is 6x6x64-ish, reshape to a vector\n",
    "            hidden4 = tf.reshape(hidden4, [-1, np.prod([int(dim) for dim in hidden4.get_shape()[1:]])])\n",
    "        else:\n",
    "            hidden4 = tf.reduce_mean(hidden4, [1, 2])\n",
    "\n",
    "        return tf.matmul(hidden4, weights['w5']) + weights['b5']\n",
    "\n",
    "# MAML-Início\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main.py-Início\n",
    "\"\"\"\n",
    "Usage Instructions:\n",
    "    10-shot sinusoid:\n",
    "        python main.py --datasource=sinusoid --logdir=logs/sine/ --metatrain_iterations=70000 --norm=None --update_batch_size=10\n",
    "\n",
    "    10-shot sinusoid baselines:\n",
    "        python main.py --datasource=sinusoid --logdir=logs/sine/ --pretrain_iterations=70000 --metatrain_iterations=0 --norm=None --update_batch_size=10 --baseline=oracle\n",
    "        python main.py --datasource=sinusoid --logdir=logs/sine/ --pretrain_iterations=70000 --metatrain_iterations=0 --norm=None --update_batch_size=10\n",
    "\n",
    "    5-way, 1-shot omniglot:\n",
    "        python main.py --datasource=omniglot --metatrain_iterations=60000 --meta_batch_size=32 --update_batch_size=1 --update_lr=0.4 --num_updates=1 --logdir=logs/omniglot5way/\n",
    "\n",
    "    20-way, 1-shot omniglot:\n",
    "        python main.py --datasource=omniglot --metatrain_iterations=60000 --meta_batch_size=16 --update_batch_size=1 --num_classes=20 --update_lr=0.1 --num_updates=5 --logdir=logs/omniglot20way/\n",
    "\n",
    "    5-way 1-shot mini imagenet:\n",
    "        python main.py --datasource=miniimagenet --metatrain_iterations=60000 --meta_batch_size=4 --update_batch_size=1 --update_lr=0.01 --num_updates=5 --num_classes=5 --logdir=logs/miniimagenet1shot/ --num_filters=32 --max_pool=True\n",
    "\n",
    "    5-way 5-shot mini imagenet:\n",
    "        python main.py --datasource=miniimagenet --metatrain_iterations=60000 --meta_batch_size=4 --update_batch_size=5 --update_lr=0.01 --num_updates=5 --num_classes=5 --logdir=logs/miniimagenet5shot/ --num_filters=32 --max_pool=True\n",
    "\n",
    "    To run evaluation, use the '--train=False' flag and the '--test_set=True' flag to use the test set.\n",
    "\n",
    "    For omniglot and miniimagenet training, acquire the dataset online, put it in the correspoding data directory, and see the python script instructions in that directory to preprocess the data.\n",
    "\n",
    "    Note that better sinusoid results can be achieved by using a larger network.\n",
    "\"\"\"\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "try:\n",
    "    matplotlib.use('agg')\n",
    "except:\n",
    "    pass\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    plt.switch_backend('agg')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from data_generator import DataGenerator\n",
    "from maml import MAML\n",
    "from tensorflow.python.platform import flags\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "## Dataset/method options\n",
    "flags.DEFINE_string('datasource', 'sinusoid', 'sinusoid or omniglot or miniimagenet')\n",
    "flags.DEFINE_integer('num_classes', 2, 'number of classes used in classification (e.g. 5-way classification).')\n",
    "# oracle means task id is input (only suitable for sinusoid)\n",
    "flags.DEFINE_string('baseline', None, 'oracle')\n",
    "# flags.DEFINE_string('baseline', None, 'oracle, or None')\n",
    "\n",
    "## Training options\n",
    "flags.DEFINE_integer('pretrain_iterations', 0, 'number of pre-training iterations.')\n",
    "flags.DEFINE_integer('metatrain_iterations', 15000, 'number of metatraining iterations.') # 15k for omniglot, 50k for sinusoid\n",
    "flags.DEFINE_integer('meta_batch_size', 25, 'number of tasks sampled per meta-update')\n",
    "flags.DEFINE_float('meta_lr', 0.001, 'the base learning rate of the generator')\n",
    "flags.DEFINE_integer('update_batch_size', 5, 'number of examples used for inner gradient update (K for K-shot learning).')\n",
    "flags.DEFINE_float('update_lr', 1e-3, 'step size alpha for inner gradient update.') # 0.1 for omniglot\n",
    "flags.DEFINE_integer('num_updates', 1, 'number of inner gradient updates during training.')\n",
    "\n",
    "## Model options\n",
    "flags.DEFINE_string('norm', 'batch_norm', 'batch_norm, layer_norm, or None')\n",
    "flags.DEFINE_integer('num_filters', 32, 'number of filters for conv nets -- 32 for miniimagenet, 64 for omiglot.') ##ANTES 64!\n",
    "flags.DEFINE_bool('conv', True, 'whether or not to use a convolutional network, only applicable in some cases')\n",
    "flags.DEFINE_bool('max_pool', False, 'Whether or not to use max pooling rather than strided convolutions')\n",
    "flags.DEFINE_bool('stop_grad', False, 'if True, do not use second derivatives in meta-optimization (for speed)')\n",
    "\n",
    "## Logging, saving, and testing options\n",
    "flags.DEFINE_bool('log', True, 'if false, do not log summaries, for debugging code.')\n",
    "flags.DEFINE_string('logdir', '/tmp/data', 'directory for summaries and checkpoints.')\n",
    "flags.DEFINE_bool('resume', True, 'resume training if there is a model available')\n",
    "flags.DEFINE_bool('train', True, 'True to train, False to test.')\n",
    "flags.DEFINE_integer('test_iter', -1, 'iteration to load model (-1 for latest model)')\n",
    "flags.DEFINE_bool('test_set', False, 'Set to true to test on the the test set, False for the validation set.')\n",
    "flags.DEFINE_integer('train_update_batch_size', -1, 'number of examples used for gradient update during training (use if you want to test with a different number).')\n",
    "flags.DEFINE_float('train_update_lr', -1, 'value of inner gradient step step during training. (use if you want to test with a different value)') # 0.1 for omniglot\n",
    "\n",
    "def train(model, saver, sess, exp_string, data_generator, resume_itr=0):\n",
    "    print('\\n ** Starting main.py training.')\n",
    "    SUMMARY_INTERVAL = 100\n",
    "    SAVE_INTERVAL = 1000\n",
    "    if FLAGS.datasource == 'sinusoid':\n",
    "        PRINT_INTERVAL = 1000\n",
    "        TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
    "    else:\n",
    "        PRINT_INTERVAL = 100\n",
    "        TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
    "\n",
    "    if FLAGS.log:\n",
    "        train_writer = tf.summary.FileWriter(FLAGS.logdir + '/' + exp_string, sess.graph)\n",
    "    print(' ** Done initializing, starting training.')\n",
    "    prelosses, postlosses = [], []\n",
    "\n",
    "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
    "    multitask_weights, reg_weights = [], []\n",
    "    iterlist, histpreloss, histposloss = [], [], []\n",
    "\n",
    "    for itr in range(resume_itr, FLAGS.pretrain_iterations + FLAGS.metatrain_iterations):\n",
    "        feed_dict = {}\n",
    "        if 'generate' in dir(data_generator):\n",
    "            batch_x, batch_y, amp, phase = data_generator.generate()\n",
    "\n",
    "            if FLAGS.baseline == 'oracle':\n",
    "                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)\n",
    "                for i in range(FLAGS.meta_batch_size):\n",
    "                    batch_x[i, :, 1] = amp[i]\n",
    "                    batch_x[i, :, 2] = phase[i]\n",
    "\n",
    "            inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            inputb = batch_x[:, num_classes*FLAGS.update_batch_size:, :] # b used for testing\n",
    "            labelb = batch_y[:, num_classes*FLAGS.update_batch_size:, :]\n",
    "            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb}\n",
    "\n",
    "        if itr < FLAGS.pretrain_iterations:\n",
    "            input_tensors = [model.pretrain_op]\n",
    "        else:\n",
    "            input_tensors = [model.metatrain_op]\n",
    "\n",
    "        if (itr % SUMMARY_INTERVAL == 0 or itr % PRINT_INTERVAL == 0):\n",
    "            input_tensors.extend([model.summ_op, model.total_loss1, model.total_losses2[FLAGS.num_updates-1]])\n",
    "            if model.classification:\n",
    "                input_tensors.extend([model.total_accuracy1, model.total_accuracies2[FLAGS.num_updates-1]])\n",
    "\n",
    "        result = sess.run(input_tensors, feed_dict)\n",
    "\n",
    "        if itr % SUMMARY_INTERVAL == 0:\n",
    "            prelosses.append(result[-2])\n",
    "            if FLAGS.log:\n",
    "                train_writer.add_summary(result[1], itr)\n",
    "            postlosses.append(result[-1])\n",
    "\n",
    "        if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
    "            if itr < FLAGS.pretrain_iterations:\n",
    "                print_str = 'Pretrain Iteration ' + str(itr)\n",
    "            else:\n",
    "                print_str = 'Iteration ' + str(itr - FLAGS.pretrain_iterations)\n",
    "            print_str += ': ' + str(np.mean(prelosses)) + ', ' + str(np.mean(postlosses))\n",
    "            #print(print_str)\n",
    "            ########\n",
    "            iterlist = np.append(iterlist, str(itr - FLAGS.pretrain_iterations))\n",
    "            histpreloss = np.append(histpreloss, np.mean(prelosses))\n",
    "            histposloss = np.append(histposloss, np.mean(postlosses))\n",
    "            #print(iterlist, histpreloss, histposloss)\n",
    "            ########\n",
    "            prelosses, postlosses = [], []\n",
    "\n",
    "        if (itr!=0) and itr % SAVE_INTERVAL == 0:\n",
    "            saver.save(sess, FLAGS.logdir + '/' + exp_string + '/model' + str(itr))\n",
    "\n",
    "        # sinusoid is infinite data, so no need to test on meta-validation set.\n",
    "        if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0 and FLAGS.datasource !='sinusoid':\n",
    "            if 'generate' not in dir(data_generator):\n",
    "                feed_dict = {}\n",
    "                if model.classification:\n",
    "                    input_tensors = [model.metaval_total_accuracy1, model.metaval_total_accuracies2[FLAGS.num_updates-1], model.summ_op]\n",
    "                else:\n",
    "                    input_tensors = [model.metaval_total_loss1, model.metaval_total_losses2[FLAGS.num_updates-1], model.summ_op]\n",
    "            else:\n",
    "                batch_x, batch_y, amp, phase = data_generator.generate(train=False)\n",
    "                inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "                inputb = batch_x[:, num_classes*FLAGS.update_batch_size:, :]\n",
    "                labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "                labelb = batch_y[:, num_classes*FLAGS.update_batch_size:, :]\n",
    "                feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}\n",
    "                if model.classification:\n",
    "                    input_tensors = [model.total_accuracy1, model.total_accuracies2[FLAGS.num_updates-1]]\n",
    "                else:\n",
    "                    input_tensors = [model.total_loss1, model.total_losses2[FLAGS.num_updates-1]]\n",
    "\n",
    "            result = sess.run(input_tensors, feed_dict)\n",
    "            print('Validation results: ' + str(result[0]) + ', ' + str(result[1]))\n",
    "\n",
    "    ####################33\n",
    "    lis = range(0,len(histpreloss),1)\n",
    "    #plt.figure()\n",
    "    #plt.ylim([0,1])\n",
    "    x2 = np.arange(len(iterlist))\n",
    "    plt.plot(x2, histpreloss)\n",
    "    plt.plot(x2, histposloss)\n",
    "    plt.xticks(x2, iterlist)\n",
    "    plt.xticks(rotation=90)\n",
    "    ax = plt.gca()\n",
    "    for label in ax.get_xaxis().get_ticklabels()[::2]:\n",
    "        label.set_visible(False)\n",
    "    plt.title('Mean Loss for Meta-train Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.legend(['Pre Loss', 'Post Loss'], loc='upper left')\n",
    "    plt.savefig(\"meta_loss_01.png\")\n",
    "    ######################3\n",
    "\n",
    "    saver.save(sess, FLAGS.logdir + '/' + exp_string +  '/model' + str(itr))\n",
    "\n",
    "# calculated for omniglot\n",
    "NUM_TEST_POINTS = 600\n",
    "\n",
    "def test(model, saver, sess, exp_string, data_generator, test_num_updates=None):\n",
    "    num_classes = data_generator.num_classes # for classification, 1 otherwise\n",
    "\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "\n",
    "    metaval_accuracies = []\n",
    "\n",
    "    for _ in range(NUM_TEST_POINTS):\n",
    "        if 'generate' not in dir(data_generator):\n",
    "            feed_dict = {}\n",
    "            feed_dict = {model.meta_lr : 0.0}\n",
    "        else:\n",
    "            batch_x, batch_y, amp, phase = data_generator.generate(train=False)\n",
    "\n",
    "            if FLAGS.baseline == 'oracle': # NOTE - this flag is specific to sinusoid\n",
    "                batch_x = np.concatenate([batch_x, np.zeros([batch_x.shape[0], batch_x.shape[1], 2])], 2)\n",
    "                batch_x[0, :, 1] = amp[0]\n",
    "                batch_x[0, :, 2] = phase[0]\n",
    "\n",
    "            inputa = batch_x[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            inputb = batch_x[:,num_classes*FLAGS.update_batch_size:, :]\n",
    "            labela = batch_y[:, :num_classes*FLAGS.update_batch_size, :]\n",
    "            labelb = batch_y[:,num_classes*FLAGS.update_batch_size:, :]\n",
    "\n",
    "            feed_dict = {model.inputa: inputa, model.inputb: inputb,  model.labela: labela, model.labelb: labelb, model.meta_lr: 0.0}\n",
    "\n",
    "        if model.classification:\n",
    "            result = sess.run([model.metaval_total_accuracy1] + model.metaval_total_accuracies2, feed_dict)\n",
    "        else:  # this is for sinusoid\n",
    "            result = sess.run([model.total_loss1] +  model.total_losses2, feed_dict)\n",
    "        print(result)\n",
    "        metaval_accuracies.append(result)\n",
    "\n",
    "    metaval_accuracies = np.array(metaval_accuracies)\n",
    "    means = np.mean(metaval_accuracies, 0)\n",
    "    stds = np.std(metaval_accuracies, 0)\n",
    "    ci95 = 1.96*stds/np.sqrt(NUM_TEST_POINTS)\n",
    "    #try:\n",
    "    #    highdev = means+stds\n",
    "    #    lowdev = means-stds\n",
    "    #except:\n",
    "    #s    pass\n",
    "\n",
    "    print(' ** Mean validation accuracy/loss, stddev, and confidence intervals')\n",
    "    print((means, stds, ci95))\n",
    "    ####################33\n",
    "    lis = range(0,len(means),1)\n",
    "    x2 = np.arange(len(lis))\n",
    "    #plt.figure()\n",
    "    plt.xlim([0,len(means)])\n",
    "    plt.ylim([0,1])\n",
    "    plt.plot(x2, means)\n",
    "    plt.title('Mean Validation Accuracy for Meta-train Iterations')\n",
    "    plt.xticks(x2, lis)\n",
    "    plt.xticks(rotation=90)\n",
    "    ax = plt.gca()\n",
    "    for label in ax.get_xaxis().get_ticklabels()[::2]:\n",
    "        label.set_visible(False)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.savefig(\"meta_val_01.png\")\n",
    "    ######################3\n",
    "\n",
    "    out_filename = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.csv'\n",
    "    out_pkl = FLAGS.logdir +'/'+ exp_string + '/' + 'test_ubs' + str(FLAGS.update_batch_size) + '_stepsize' + str(FLAGS.update_lr) + '.pkl'\n",
    "    with open(out_pkl, 'wb') as f:\n",
    "        pickle.dump({'mses': metaval_accuracies}, f)\n",
    "    with open(out_filename, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter=',')\n",
    "        writer.writerow(['update'+str(i) for i in range(len(means))])\n",
    "        writer.writerow(means)\n",
    "        writer.writerow(stds)\n",
    "        writer.writerow(ci95)\n",
    "\n",
    "def main():\n",
    "    print('\\n ** Initializing main.py program...')\n",
    "    if FLAGS.datasource == 'sinusoid':\n",
    "        if FLAGS.train:\n",
    "            test_num_updates = 5\n",
    "        else:\n",
    "            test_num_updates = 10\n",
    "    else:\n",
    "        if FLAGS.datasource == 'miniimagenet':\n",
    "            if FLAGS.train == True:\n",
    "                test_num_updates = 1  # eval on at least one update during training\n",
    "            else:\n",
    "                test_num_updates = 10\n",
    "        else:\n",
    "            test_num_updates = 10\n",
    "\n",
    "    if FLAGS.train == False:\n",
    "        orig_meta_batch_size = FLAGS.meta_batch_size\n",
    "        # always use meta batch size of 1 when testing.\n",
    "        FLAGS.meta_batch_size = 1\n",
    "\n",
    "    if FLAGS.datasource == 'sinusoid':\n",
    "        data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)\n",
    "    else:\n",
    "        print(' ** Importing data_generator from DataGenerator class.')\n",
    "        if FLAGS.metatrain_iterations == 0 and FLAGS.datasource == 'miniimagenet':\n",
    "            print(' ** Importing when not in metatrain_iterations.')\n",
    "            assert FLAGS.meta_batch_size == 1\n",
    "            assert FLAGS.update_batch_size == 1\n",
    "            data_generator = DataGenerator(1, FLAGS.meta_batch_size)  # only use one datapoint,\n",
    "        else:\n",
    "            if FLAGS.datasource == 'miniimagenet': # TODO - use 15 val examples for imagenet?\n",
    "                print(' ** Importing when not in metatrain_iterations.')\n",
    "                if FLAGS.train:\n",
    "                    data_generator = DataGenerator(FLAGS.update_batch_size+15, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "                    print(' ** data_generator: ', data_generator)\n",
    "                else:\n",
    "                    data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "                    print(' ** data_generator: ', data_generator)\n",
    "            else:\n",
    "                data_generator = DataGenerator(FLAGS.update_batch_size*2, FLAGS.meta_batch_size)  # only use one datapoint for testing to save memory\n",
    "\n",
    "    print(' ** Finished data importion.')\n",
    "    dim_output = data_generator.dim_output\n",
    "    print(' ** dim_output:', dim_output)\n",
    "    if FLAGS.baseline == 'oracle':\n",
    "        assert FLAGS.datasource == 'sinusoid'\n",
    "        dim_input = 3\n",
    "        FLAGS.pretrain_iterations += FLAGS.metatrain_iterations\n",
    "        FLAGS.metatrain_iterations = 0\n",
    "    else:\n",
    "        dim_input = data_generator.dim_input\n",
    "        print(' ** dim_input:', dim_output)\n",
    "\n",
    "    if FLAGS.datasource == 'miniimagenet' or FLAGS.datasource == 'omniglot':\n",
    "        tf_data_load = True\n",
    "        num_classes = data_generator.num_classes\n",
    "        print(' ** num_classes:', num_classes)\n",
    "\n",
    "        if FLAGS.train: # only construct training model if needed\n",
    "            random.seed(5)\n",
    "            image_tensor, label_tensor = data_generator.make_data_tensor()\n",
    "            inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "            inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "            labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "            labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "            input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
    "\n",
    "        random.seed(6)\n",
    "        image_tensor, label_tensor = data_generator.make_data_tensor(train=False)\n",
    "        inputa = tf.slice(image_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "        inputb = tf.slice(image_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "        labela = tf.slice(label_tensor, [0,0,0], [-1,num_classes*FLAGS.update_batch_size, -1])\n",
    "        labelb = tf.slice(label_tensor, [0,num_classes*FLAGS.update_batch_size, 0], [-1,-1,-1])\n",
    "        metaval_input_tensors = {'inputa': inputa, 'inputb': inputb, 'labela': labela, 'labelb': labelb}\n",
    "    else:\n",
    "        tf_data_load = False\n",
    "        input_tensors = None\n",
    "\n",
    "    model = MAML(dim_input, dim_output, test_num_updates=test_num_updates)\n",
    "    if FLAGS.train or not tf_data_load:\n",
    "        model.construct_model(input_tensors=input_tensors, prefix='metatrain_')\n",
    "    if tf_data_load:\n",
    "        model.construct_model(input_tensors=metaval_input_tensors, prefix='metaval_')\n",
    "    model.summ_op = tf.summary.merge_all()\n",
    "\n",
    "    saver = loader = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES), max_to_keep=10)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    if FLAGS.train == False:\n",
    "        # change to original meta batch size when loading model.\n",
    "        FLAGS.meta_batch_size = orig_meta_batch_size\n",
    "\n",
    "    if FLAGS.train_update_batch_size == -1:\n",
    "        FLAGS.train_update_batch_size = FLAGS.update_batch_size\n",
    "    if FLAGS.train_update_lr == -1:\n",
    "        FLAGS.train_update_lr = FLAGS.update_lr\n",
    "\n",
    "    exp_string = 'cls_'+str(FLAGS.num_classes)+'.mbs_'+str(FLAGS.meta_batch_size) + '.ubs_' + str(FLAGS.train_update_batch_size) + '.numstep' + str(FLAGS.num_updates) + '.updatelr' + str(FLAGS.train_update_lr)\n",
    "\n",
    "    if FLAGS.num_filters != 64:\n",
    "        exp_string += 'hidden' + str(FLAGS.num_filters)\n",
    "    if FLAGS.max_pool:\n",
    "        exp_string += 'maxpool'\n",
    "    if FLAGS.stop_grad:\n",
    "        exp_string += 'stopgrad'\n",
    "    if FLAGS.baseline:\n",
    "        exp_string += FLAGS.baseline\n",
    "    if FLAGS.norm == 'batch_norm':\n",
    "        exp_string += 'batchnorm'\n",
    "    elif FLAGS.norm == 'layer_norm':\n",
    "        exp_string += 'layernorm'\n",
    "    elif FLAGS.norm == 'None':\n",
    "        exp_string += 'nonorm'\n",
    "    else:\n",
    "        print(' ** Norm setting not recognized.')\n",
    "\n",
    "    resume_itr = 0\n",
    "    model_file = None\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.train.start_queue_runners()\n",
    "\n",
    "    if FLAGS.resume or not FLAGS.train:\n",
    "        model_file = tf.train.latest_checkpoint(FLAGS.logdir + '/' + exp_string)\n",
    "        if FLAGS.test_iter > 0:\n",
    "            model_file = model_file[:model_file.index('model')] + 'model' + str(FLAGS.test_iter)\n",
    "        if model_file:\n",
    "            ind1 = model_file.index('model')\n",
    "            resume_itr = int(model_file[ind1+5:])\n",
    "            print(\"\\n ** Restoring model weights from \" + model_file)\n",
    "            saver.restore(sess, model_file)\n",
    "\n",
    "    if FLAGS.train:\n",
    "        train(model, saver, sess, exp_string, data_generator, resume_itr)\n",
    "    else:\n",
    "        test(model, saver, sess, exp_string, data_generator, test_num_updates)\n",
    "    print('\\n ** FINISHED! *************************')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Main.py-Início\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}